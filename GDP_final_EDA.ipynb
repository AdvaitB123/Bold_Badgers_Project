{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains the average GDP per capita of 227 countries from 1970 to 2017 as well as the average of other metrics such as Infant mortality (per 1000 births) and Literacy % for each country\n",
    "\n",
    "We want our model to be able to predict the GDP per capita for a country based on inputted values for the other metrics.\n",
    "\n",
    "Here is a link to our dataset: https://www.kaggle.com/code/noxmoon/world-countries-predicting-gdp/input\n",
    "\n",
    "Here is some more specific information on the columns:\n",
    "\n",
    "Arable %: Percentage of land capable of cultivating crops\n",
    "<br>\n",
    "Crop %: percentage of arable land that is actually under cultivation and used for growing crops\n",
    "<br>\n",
    "Other %: percentage of arable land that is not used for crops\n",
    "<br>\n",
    "Agriculture: percentage of GDP that comes from agriculture\n",
    "<br>\n",
    "Industry: percentage of GDP that comes from industrial activities\n",
    "<br>\n",
    "Service: percentage of GDP from the service industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_matrix(\n",
    "    df, \n",
    "    dimensions=[\n",
    "        'GDP ($ per capita)',\n",
    "        'Net migration',\n",
    "        'Infant mortality (per 1000 births)',\n",
    "        'Literacy (%)',\n",
    "        'Phones (per 1000)',\n",
    "        'Birthrate',\n",
    "    ], \n",
    "    width= 1920, \n",
    "    height=1080,\n",
    "    color='Region', \n",
    "    hover_data='Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adf3b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = 'countries of the world.csv'\n",
    "\n",
    "df = pd.read_csv(file, decimal = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fc6fb",
   "metadata": {},
   "source": [
    "# Preprocessing for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills null values with the average column value by region \n",
    "\n",
    "for col in df.columns.values:\n",
    "  if df[col].isnull().sum() == 0: # if column does not have null values\n",
    "    continue \n",
    "  #otherwise\n",
    "  med_values = df.groupby('Region')[col].median()\n",
    "  for region in df['Region'].unique():\n",
    "    # replace value where value is null and df[Region] = region\n",
    "    df[col].loc[(df[col].isnull()) & (df['Region'] == region)] = med_values[region] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data before dummy variables to use for data analysis\n",
    "\n",
    "graph_data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a7f44",
   "metadata": {},
   "source": [
    "# Data Analysis and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552920ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min column values with country name\n",
    "\n",
    "for col in df.columns.values[2:]:\n",
    "  index_of_min = df[col].idxmin()\n",
    "  min_country = df['Country'].loc[index_of_min]\n",
    "  min_value = df[col].min()\n",
    "  print(col + \", \" + min_country + \", \" + str(min_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max column values with country name\n",
    "\n",
    "for col in df.columns.values[2:]:\n",
    "  index_of_max = df[col].idxmax()\n",
    "  max_country = df['Country'].loc[index_of_max]\n",
    "  max_value = df[col].max()\n",
    "  print(col + \", \" + max_country + \", \" + str(max_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a correlation map of all columns in the dataframe \n",
    "<br>\n",
    "Values close to -1 or 1 are a good indicator of important features in the dataset \n",
    "<br>\n",
    "For example, GDP is highly correlated to Phones per 1000 people (0.83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "\n",
    "import plotly_express as px\n",
    "\n",
    "corr = graph_data[graph_data.columns.values[2:]].corr() # we only want to take the columns that have numerical data, so we leave out the country and region columns\n",
    "fig= px.imshow(corr, text_auto=True, width=1300, height=1300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "px.scatter(\n",
    "    df, \n",
    "    x=\"GDP ($ per capita)\",\n",
    "    y=\"Phones (per 1000)\", \n",
    "    trendline=\"ols\", \n",
    "    hover_data=\"Country\",\n",
    "    log_x=False,trendline_options=dict(log_x=True))\n",
    "\n",
    "# Most of the features in the dataset are logarithmically correlated with GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ec057",
   "metadata": {},
   "source": [
    "Markdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b97f5",
   "metadata": {},
   "source": [
    "Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1715d2",
   "metadata": {},
   "source": [
    "# Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1020a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummies for Region \n",
    "\n",
    "dummies = pd.get_dummies(df['Region'])\n",
    "\n",
    "df['Asia'] = dummies['ASIA (EX. NEAR EAST)         ']\n",
    "df['Eastern Europe'] = dummies['EASTERN EUROPE                     ']\n",
    "df['Northern Africa'] = dummies['NORTHERN AFRICA                    ']\n",
    "df['Oceania'] = dummies['OCEANIA                            ']\n",
    "df['Western Europe'] = dummies['WESTERN EUROPE                     ']\n",
    "df['Sub-Saharan Africa'] = dummies['SUB-SAHARAN AFRICA                 ']\n",
    "df['Latin America and Caribbean'] = dummies['LATIN AMER. & CARIB    ']\n",
    "df['Commonwealth of Independent States'] = dummies['C.W. OF IND. STATES ']\n",
    "df['Near East'] = dummies['NEAR EAST                          ']\n",
    "df['North America'] = dummies['NORTHERN AMERICA                   ']\n",
    "df['Baltics'] = dummies['BALTICS                            ']\n",
    "\n",
    "df.drop('Region', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4589d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot use for prediction\n",
    "\n",
    "df.drop('Country', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "target = df['GDP ($ per capita)']\n",
    "features = df.loc[:, df.columns != 'GDP ($ per capita)']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 21) # random state to keep splits the same in different runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a795889",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32527e16",
   "metadata": {},
   "source": [
    "### Description of Decision Trees\n",
    "\n",
    "A decision tree is a ML model that utilize “if-then” logic. The decision nodes (eg “Outlook”) represent a decision that needs to be made or a question that needs to be answered. The branches associated with a decision node represent the possible outcomes of that decision (eg “Sunny”, “Overcast”, “Rainy”). Eventually, the branches lead to leaf nodes which represent the final decision made or outcome. \n",
    "\n",
    "![Alt text](https://www.saedsayad.com/images/Decision_tree_r1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'friedman_mse', 'max_depth': 60, 'min_samples_leaf': 2, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "criterion = ['squared_error', 'friedman_mse', 'absolute_error', 'poisson']\n",
    "\n",
    "max_depth = [10, 20, 30, 40, 50, 60, 70]\n",
    "\n",
    "min_samples_split = [2, 3, 4, 5, 6]\n",
    "\n",
    "min_samples_leaf = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "param_grid = {\n",
    "    'criterion' : criterion,\n",
    "    'max_depth' : max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf' : min_samples_leaf\n",
    "    \n",
    "}\n",
    "\n",
    "dt_grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, n_jobs = -1)\n",
    "dt_grid_search.fit(x_train, y_train)\n",
    "print(dt_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "my_decision_tree = DecisionTreeRegressor(criterion = 'friedman_mse', max_depth = 60, min_samples_split=2, min_samples_leaf=5)\n",
    "\n",
    "my_decision_tree.fit(x_train, y_train)\n",
    "\n",
    "feature_importance = my_decision_tree.feature_importances_\n",
    "\n",
    "my_features = (df.iloc[:, df.columns != 'GDP ($ per capita)'].columns)\n",
    "\n",
    "def sortSecond(val):\n",
    "\treturn val[1]\n",
    "\n",
    "feature_order = []\n",
    "importances = [] \n",
    "\n",
    "importances = [(my_features[i], feature_importance[i]) for i in range(len(my_features))]\n",
    "importances.sort(reverse=True, key=sortSecond)\n",
    "\n",
    "# select the top 10 features from your importances array \n",
    "top_dt_features = [feature[0] for feature in importances[:12]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(criterion=&#x27;poisson&#x27;, max_depth=50, min_samples_leaf=5,\n",
       "                      min_samples_split=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(criterion=&#x27;poisson&#x27;, max_depth=50, min_samples_leaf=5,\n",
       "                      min_samples_split=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(criterion='poisson', max_depth=50, min_samples_leaf=5,\n",
       "                      min_samples_split=5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Optimized Decision Tree Model\n",
    "\n",
    "\n",
    "dt_features = df[top_dt_features]\n",
    "\n",
    "x_train_dt, x_test_dt, y_train_dt, y_test_dt = train_test_split(dt_features, target, test_size = 0.2)\n",
    "\n",
    "my_decision_tree = DecisionTreeRegressor(criterion = 'poisson', max_depth = 50, min_samples_split=5, min_samples_leaf=5)\n",
    "\n",
    "my_decision_tree.fit(x_train_dt, y_train_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01701e4",
   "metadata": {},
   "source": [
    "R^2 shows how well the data fit the regression model (the goodness of fit). The value of R^2 can range from 0 to 1, where 0 means that our model does not predict any variability in our data and 1 means that our model prefectly fits/predicts our data.\n",
    "\n",
    "MAE, or the Mean Absolute Error, is way of seeing how accurate our model is. MAE more intuitive and easy to interpret than R^2. Our Mean Squared Error represents, on average, how much our predicted value differed from the real value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81263bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score:  0.8390501844149421\n",
      "mae:  2944.678226363009\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Decision Tree Model\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "y_hat_dt = my_decision_tree.predict(x_test_dt)\n",
    "\n",
    "r2_dt = r2_score(y_test_dt, y_hat_dt)\n",
    "\n",
    "mae_dt = mean_absolute_error(y_test_dt, y_hat_dt)\n",
    "\n",
    "print('r2 score: ', r2_dt)\n",
    "print('mae: ', mae_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499cab5",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa53f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of model (how it works)\n",
    "\n",
    "# Random Forests clasify data into smaller classes each with unique values and features, and then trains these subsections using Decision Trees. \n",
    "# It starts with data and for every node it asks specific type of question, which will classify given data.\n",
    "# After all iterations through tree of questions, random forest is trained and has classified data. \n",
    "\n",
    "# Hyperparameter tuning / feature selection\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Model performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b218dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89f9d7c3",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade98197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of model (how it works) \n",
    "#Stochastic gradient decent is an optimization alogrothim used to train machine learning models\n",
    "#Makes sure that the model doesn't make mistakes such as overfitting\n",
    "'''\n",
    "Steps Include:\n",
    "1) Initialize Parameters --> Random variables for the model's paramters (weights, baises etc)\n",
    "2) Shuffle Data --> To get rid of baises \n",
    "3) Mini-Batch Detection --> Dividng data into small batches for better training\n",
    "4) Paramter Update --> For each mini-batch, compute gradient of the loss function and update parameters as such\n",
    "5) Iterate --> Repeat steps 3 & 4 for certain number of iteration (epochs)\n",
    "'''\n",
    "\n",
    "# Hyperparameter tuning / feature scaling\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Model performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceaf9f0",
   "metadata": {},
   "source": [
    "# MLP Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44e0d7",
   "metadata": {},
   "source": [
    "### Description of a MLP NN\n",
    "\n",
    "An MLP (Multi Layer Perceptron) Neural network works by using different \"nodes\" in various layers. Many neural networks have three \"hidden\" layers, and an input and output layer. Each layer will take an input of N features and output some number of features. Each layer must have the same input size as the output of the layer above, or in the case of the input layer, the same number of inputs as features being used. \n",
    "\n",
    "Each \"node\" in a neural network uses an activation function, which determines if the node is activated or disabled. Most commonly and in our case, the ReLU activation function is used. Each node will have a weight attached to it that determines the activation function parameters as well as the activation threshold of the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29569fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn MLPRegressor model was difficult to work with and customize so we moved to pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
